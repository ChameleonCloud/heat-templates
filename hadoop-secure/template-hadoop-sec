# This describes what is deployed by this template.
description: Hadoop master and workers deployed with Heat on Chameleon

# This defines the minimum Heat version required by this template.
heat_template_version: 2015-10-15

# The resources section defines what OpenStack resources are to be deployed and
# how they should be configured.
resources:
  router_to_ext:
    type: OS::Neutron::Router
    properties:
      name: hadoop_router
      external_gateway_info:
        network: public

  hadoop_subnet:
    type: OS::Neutron::Subnet
    properties:
      name: hadoop_subnet
      cidr: { get_param: network_cidr }
      enable_dhcp: true
      network: { get_resource: hadoop_network }

  hadoop_network:
    type: OS::Neutron::Net
    properties:
      name: { get_param: network_name }

  hadoop_router_interface:
    type: OS::Neutron::RouterInterface
    properties:
      router: { get_resource: router_to_ext }
      subnet: { get_resource: hadoop_subnet }

  hadoop_master_floating_ip:
    type: OS::Nova::FloatingIP
    properties:
      pool: public

  hadoop_workers:
    type: OS::Heat::ResourceGroup
    properties:
      count: { get_param: hadoop_worker_count }
      resource_def:
        type: OS::Nova::Server
        properties:
          name: worker-%index%
          flavor: baremetal
          image: CC-CentOS7
          key_name: { get_param: key_name }
          networks:
             - network: { get_resource: hadoop_network }
          scheduler_hints: { reservation: { get_param: reservation_id } }
          user_data:
            str_replace:
              template: |
                #!/bin/bash

                exec &>/boot.log

                echo nameserver 8.8.8.8 >> /etc/resolv.conf  

                MY_IP=`ifconfig eno1 | grep netmask | tr -s ' ' | cut -d " " -f 3`
                #FQDN=host-"MY_IP".openstacklocal
              
                CWD=`pwd`
                cd /usr/lib
                mkdir -m 777 hadoop

                #Download hadoop
                wget -r --no-parent -nH --cut-dirs=3 -A 'hadoop-*.*.*.tar.gz' -R 'hadoop-*.*.*-*' http://apache.javapipe.com/hadoop/common/stable/            
                HADOOP_DL=$(ls -t hadoop-*.tar.gz | head -n1)
                HADOOP_LATEST="${HADOOP_DL::-7}" 

                HADOOP_CONF_DIR=/usr/lib/${HADOOP_LATEST}/etc/hadoop
                CORE_SITE_FILE=${HADOOP_CONF_DIR}/core-site.xml
                HDFS_SITE_FILE=${HADOOP_CONF_DIR}/hdfs-site.xml
                MAPRED_SITE_FILE=${HADOOP_CONF_DIR}/mapred-site.xml
                YARN_SITE_FILE=${HADOOP_CONF_DIR}/yarn-site.xml
                WORKERS_FILE=${HADOOP_CONF_DIR}/workers

                #Create and configure users

                cat > users <<EOF
                hadoop
                hdfs
                yarn
                mapred
                EOF
    
                cat users | while read line; do
    
                  useradd "$line"
      
                  mkdir /home/"$line"/.ssh
                  echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCpLETezO6hUHgiLjHPEXXN6kkV9vBFqYAc4ha6OOoUYztx66mC3Sb590DZvn1wbUFZTJHMqRVG4x08MAsNBpBnuFsGQCg7Rw7cpW8uA20kJpAAUFtTJZO+gSu41QFMSLpX34tTqXBC7HMzmHZOGPtMzgt8fj2IhkZXq7o3mFWDct0GM7j5ShT3nzFkG8FTalLhPk/htRu3XYojOuWZJoVS0ZGVCkHuP2IJ0EzFEhDaplrXTVTujIEOpdCehlupFcWTCaZ/p8vil646M+JYX9pj6ijihn1XfC/c0w3cO+3neOaBhA+bvISvELI/JOcuUfCHsTv626Fpjw59wv6VcInH $line@Node0" >> /home/"$line"/.ssh/authorized_keys
      
                  echo ""  > /home/"$line"/.ssh/id_rsa
                  cat > /home/"$line"/.ssh/id_rsa  <<EOF
                -----BEGIN RSA PRIVATE KEY-----
                MIIEogIBAAKCAQEAqSxE3szuoVB4Ii4xzxF1zepJFfbwRamAHOIWujjqFGM7ceup
                gt0m+fdA2b59cG1BWUyRzKkVRuMdPDALDQaQZ7hbBkAoO0cO3KVvLgNtJCaQAFBb
                UyWTvoEruNUBTEi6V9+LU6lwQuxzM5h2Thj7TM4LfH49iIZGV6u6N5hVg3LdBjO4
                +UoU958xZBvBU2pS4T5P4bUbt12KIzrlmSaFUtGRlQpB7j9iCdBMxRIQ2qZa101U
                7oyBDqXQnoZbqRXFkwmmf6fL4peuOjPiWF/aY+oo4oZ9V3wv3NMN3Dvt53jmgYQP
                m7yErxCyPyTnLlHwh7E7+tuhaY8OfcL+lXCJxwIDAQABAoIBADgng6zZJZTSWy4t
                W0c6qnnxfNUXpOXav7XWrmieH8Uos0C7UwcnVZq/of0lKAo7meeEbRkcPv3KwZeK
                8wAd360uGrjWbwROL/a5y0/gv0eyrTYNdmMBJCumQNcXjVi/A2vLvjnFEoiEaDEG
                OK7vx+rUside2BoLSCotzKBLpob8/YyEM0HrKUw/R31SNCKg3IoP49RApXj3ReEk
                8Jry3CzR48vjzu2TnZ74V/V4IN+goehcRTNHX00k6eQj7GhYV5b0TdblAaXZcKjB
                809PQHrUHS5Ub21KB1o4NyQkqIfDJ1R8jdYxrJniFp2fCfW9oj8xv53qmM4B7dXo
                zDWThDECgYEA0GyBNdZNAWbb5e+3hYomyVT0H4N0iyT9MARG3k9SF+TJOtdrWxH9
                2FNXuMiktD/abXp8Coqy0RpsdgyDnpoVr7R0l5jULtTBFx/xrZRNoryHpb/QbuZ7
                fKdWgEDjmFphB1clXUz5Bj0Y+8nIwEns18ULSswGOhzRe9fsM1xiofMCgYEAz8oX
                D27K9EUKhHmewAxUVAQ5RJV2OtXGQKZwlVo3kixRz4eQOrPWpbau6VpboppzCZ4U
                m8JLDYzaEXqeQd5XwebSMfS/+mkxGnRAsvNZIjArIdU8SNZUyUheDuKjPbROyquD
                RHr4Ro15FW3hLog7rHZAL9bmpZeZfoDDNegeGd0CgYBlMTkutWRf2NvM8K0uxdt9
                BqUcI8vSvtu6k2kBCIv4E9lrmymBZuPTQuulSK1G4nWfj8dnqt2Uznp4eizxNShw
                TXIKJGZocl1pZ9YEC6wB5f0KCW4eWgL8i5Zg4KBf2Qmg8buvZ+7EC6f0n4y7Z2j5
                fa602wfu8Qz4TuZcLW+p5wKBgCLCSJdBTlwMTJUajy7LITQovLe3VN7EsfRQo1ao
                j9E47rqLj9nyCX8RDzNj9R4/Pe0m74Wau9lZbYUtANo96mo6RYEr0w19mUQ2nDgT
                Mx7f9ecj94CrseU14N4WlX4V8nQ+uqey9mM++TlXdyrEiU7xPQ2DonOi539c5MrY
                uGhVAoGADSYen+9Z2Diwp7NLh3RwJgyccodyQiK5zmZ9U1nKgt5Gush5mnVCzS6i
                CMBg/5oEw/+R5t0OoVOsI2/S2GA3RNhzGfzLOtiv3pzah9Ma9J6yeH4PwBB8SICl
                jlHv/FXpW8SqRseSJlDCQoXTamhROXK8sKQ18Eb6gvKMRMDYERM=
                -----END RSA PRIVATE KEY-----
                EOF
                  
                  echo ""  > /home/"$line"/.ssh/config
                  cat > /home/"$line"/.ssh/config <<EOF
                Host `echo $MY_IP  | sed 's/.[0-9][0-9]*$//g'`.* 0.0.0.0 host-*
                   StrictHostKeyChecking no
                   UserKnownHostsFile=/dev/null
                EOF

                  chown -R "$line":"$line" /home/"$line"/.ssh
                  chmod 600 /home/"$line"/.ssh/*
                  
                  echo "export JAVA_HOME=/usr/lib/jvm/jre" >> /home/"$line"/.bashrc
                  echo "export HADOOP_HOME=/usr/lib/${HADOOP_LATEST}" >> /home/"$line"/.bashrc
                  echo "export HADOOP_INSTALL=\$HADOOP_HOME" >> /home/"$line"/.bashrc
                  echo "export HADOOP_MAPRED_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
                  echo "export HADOOP_COMMON_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
                  echo "export HADOOP_HDFS_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
                  echo "export HADOOP_YARN_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
                  echo "export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_HOME/lib/native" >> /home/"$line"/.bashrc
                  echo "export PATH=\$PATH:\$HADOOP_HOME/sbin:\$HADOOP_HOME/bin" >> /home/"$line"/.bashrc
                  echo "export JAVA_LIBRARY_PATH=\$HADOOP_HOME/lib/native:\$JAVA_LIBRARY_PATH" >> /home/"$line"/.bashrc
    
                  done
                
                #A better way to setup /etc/hosts, /etc/krb5.conf, and keytab files is needed
                chmod 666 /etc/hosts
                install -m 666 /dev/null /etc/krb5.conf
                install -m 666 /dev/null /etc/default/hadoop-hdfs-datanode
                install -m 770 -o hdfs -g hadoop -D /dev/null $HADOOP_CONF_DIR/conf/hdfs.keytab
                install -m 770 -o yarn -g hadoop -D /dev/null $HADOOP_CONF_DIR/conf/yarn.keytab
                install -m 770 -o mapred -g hadoop -D /dev/null $HADOOP_CONF_DIR/conf/mapred.keytab
                chmod u+s $HADOOP_CONF_DIR/conf/*.keytab && chmod g+s $HADOOP_CONF_DIR/conf/*.keytab

                #install iptables
                yum install -y iptables-services
                systemctl enable iptables
                
                # Flush all current rules from iptables#
                iptables -F
                iptables -F -t nat
                iptables --delete-chain
                
                # Set access for localhost
                iptables -A INPUT -i lo -j ACCEPT
                iptables -A OUTPUT -o lo -j ACCEPT
                
                # Subnet open access
                iptables -A INPUT -s $cidr -j ACCEPT
                iptables -A OUTPUT -s $cidr -j ACCEPT

                # web traffic
                iptables -A OUTPUT -p tcp -m tcp --dport 80 -j ACCEPT
                iptables -A OUTPUT -p tcp -m tcp --dport 443 -j ACCEPT

                # DNS
                iptables -A OUTPUT -p udp --dport 53 -m state --state NEW,ESTABLISHED -j ACCEPT
                iptables -A OUTPUT -p tcp --dport 53 -m state --state NEW,ESTABLISHED -j ACCEPT
                
                #drop icmp
                iptables -A INPUT -p icmp --icmp-type any -j DROP
                
                # Accept packets belonging to established and related connections
                iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
                
                # Allow ssh
                iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
                
                # Lock everything down
                iptables -P INPUT DROP
                iptables -P FORWARD DROP
                iptables -P OUTPUT ACCEPT

                #setup hadoop
                
                yum install -y vim tzdata-java java-1.8.0-openjdk

                tar -zxvf hadoop-*.*.*.tar.gz

                install -Dv /dev/null $HADOOP_CONF_DIR/logs/hadoop_boot.log
                HADOOP_LOG='$HADOOP_CONF_DIR/logs/hadoop_boot.log'

                wget -O /etc/yum.repos.d/bigtop.repo www.apache.org/dist/bigtop/bigtop-1.3.0/repos/centos7/bigtop.repo
                yum clean all
                yum install -y bigtop-utils

                ################################################
                #install krb client
                yum install -y krb5-workstation

                #Set permissions
                chown -R root:hadoop /usr/lib/hadoop /tmp /var
                find /usr/lib/$HADOOP_LATEST -not -path "$HADOOP_CONF_DIR/conf/*" -exec chown root:hadoop {} \;
                chmod -R 770 $HADOOP_CONF_DIR /usr/lib/hadoop /var
                chown root:root /var/empty/sshd
                chmod 700 /var/empty/sshd     
                mkdir -p /usr/lib/$HADOOP_LATEST/logs
                chmod -R 777 /usr/lib/$HADOOP_LATEST/logs
                chgrp -R hadoop /usr/lib/$HADOOP_LATEST/logs

                gpasswd -M hadoop,hdfs,yarn,mapred,cc,ccadmin hadoop       
                
                echo ""  > $HDFS_SITE_FILE
                cat > $HDFS_SITE_FILE   <<EOF
                <?xml version="1.0" encoding="UTF-8"?>
                <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
                <configuration>
                <property>
                 <name>dfs.replication</name>
                 <value>2</value>
                </property>
                <property>
                  <name>dfs.name.dir</name>
                    <value>file:///usr/lib/hadoop/hadoopdata/hdfs/namenode</value>
                </property>
                <property>
                  <name>dfs.data.dir</name>
                    <value>file:///usr/lib/hadoop/hadoopdata/hdfs/datanode</value>
                </property>
                <property>
                  <name>dfs.block.access.token.enable</name>
                  <value>true</value>
                </property>
                
                <!-- NameNode security config -->
                <property>
                  <name>dfs.namenode.keytab.file</name>
                  <value>$HADOOP_CONF_DIR/conf/hdfs.keytab</value> 
                </property>
                <property>
                  <name>dfs.namenode.kerberos.principal</name>
                  <value>hdfs/_HOST@HDPCLUSTER.LOCAL</value>
                </property>
                <property>
                  <name>dfs.namenode.kerberos.internal.spnego.principal</name>
                  <value>HTTP/_HOST@HDPCLUSTER.LOCAL</value>
                </property>
                
                <!-- Secondary NameNode security config -->
                <property>
                  <name>dfs.secondary.namenode.keytab.file</name>
                  <value>$HADOOP_CONF_DIR/conf/hdfs.keytab</value>
                </property>
                <property>
                  <name>dfs.secondary.namenode.kerberos.principal</name>
                  <value>hdfs/_HOST@HDPCLUSTER.LOCAL</value>
                </property>
                <property>
                  <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
                  <value>HTTP/_HOST@HDPCLUSTER.LOCAL</value>
                </property>
                
                <!-- DataNode security config -->
                <property>
                  <name>dfs.datanode.data.dir.perm</name>
                  <value>700</value> 
                </property>
                <property>
                  <name>dfs.datanode.address</name>
                  <value>0.0.0.0:1004</value>
                </property>
                <property>
                  <name>dfs.datanode.http.address</name>
                  <value>0.0.0.0:1006</value>
                </property>
                <property>
                  <name>dfs.datanode.keytab.file</name>
                  <value>$HADOOP_CONF_DIR/conf/hdfs.keytab</value>
                </property>
                <property>
                  <name>dfs.datanode.kerberos.principal</name>
                  <value>hdfs/_HOST@HDPCLUSTER.LOCAL</value>
                </property>
                
                <!-- Web Authentication config -->
                <property>
                  <name>dfs.web.authentication.kerberos.principal</name>
                  <value>HTTP/_HOST@HDPCLUSTER.LOCAL</value>
                 </property>
                <property>
                  <name>dfs.http.policy</name>
                  <value>HTTPS_ONLY</value>
                </property>
                </configuration>
                EOF
                
                
                echo ""  > $MAPRED_SITE_FILE
                cat > $MAPRED_SITE_FILE   <<EOF
                <configuration>
                <property>
                  <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
                <property>
                  <name>yarn.app.mapreduce.am.env</name>
                    <value>HADOOP_MAPRED_HOME=/usr/lib/$HADOOP_LATEST</value>
                </property>
                <property>
                  <name>mapreduce.map.env</name>
                    <value>HADOOP_MAPRED_HOME=/usr/lib/$HADOOP_LATEST</value>
                </property>
                <property>
                  <name>mapreduce.reduce.env</name>
                    <value>HADOOP_MAPRED_HOME=/usr/lib/$HADOOP_LATEST</value>
                </property>
                </configuration>
                EOF

                ################################################
                #Configure hadoop-hdfs-datanode
            
                cat > /etc/default/hadoop-hdfs-datanode <<EOF
                export HADOOP_SECURE_DN_USER=hdfs
                export HADOOP_SECURE_DN_PID_DIR=/var/lib/hadoop-hdfs
                export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs
                export JSVC_HOME=/usr/lib/bigtop-utils/
                EOF
                
                cat > /root/.ssh/config <<EOF
                Host `echo $MASTER_IP  | sed 's/.[0-9][0-9]*$//g'`.* 0.0.0.0 host-*
                   StrictHostKeyChecking no
                   UserKnownHostsFile=/dev/null
                EOF
                
                cd $CWD
 
              params:
                $worker_count: { get_param: hadoop_worker_count }
                $cidr: { get_param: network_cidr }

  hadoop_master:
    type: OS::Nova::Server
    properties:
      name: master
      flavor: baremetal
      image: CC-CentOS7
      key_name: { get_param: key_name }
      networks:
         - network: { get_resource: hadoop_network }
      scheduler_hints: { reservation: { get_param: reservation_id } }
      user_data:
        str_replace:
          template: |
            #!/bin/bash

            exec &>/boot.log
            echo $cidr > /tmp/cidr

            echo nameserver 8.8.8.8 >> /etc/resolv.conf

            echo $worker_ips > /tmp/client_ips.txt
            echo $worker_names > /tmp/client_names.txt

            MASTER_IP=`ifconfig eno1 | grep netmask | tr -s ' ' | cut -d " " -f 3`
            FQDN=host-"$MASTER_IP".openstacklocal
            #FQDN=master.novalocal

            #download hadoop

            CWD=`pwd`
            cd /usr/lib
            mkdir -m 777 hadoop

            wget -r --no-parent -nH --cut-dirs=3 -A 'hadoop-*.*.*.tar.gz' -R 'hadoop-*.*.*-*' http://apache.javapipe.com/hadoop/common/stable/
            HADOOP_DL=$(ls -t hadoop-*.tar.gz | head -n1)
            HADOOP_LATEST="${HADOOP_DL::-7}"   

            #Create and configure users

            cat > users <<EOF
            hadoop
            hdfs
            yarn
            mapred
            EOF

            cat users | while read line; do

              useradd "$line"
  
              mkdir /home/"$line"/.ssh
              echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCpLETezO6hUHgiLjHPEXXN6kkV9vBFqYAc4ha6OOoUYztx66mC3Sb590DZvn1wbUFZTJHMqRVG4x08MAsNBpBnuFsGQCg7Rw7cpW8uA20kJpAAUFtTJZO+gSu41QFMSLpX34tTqXBC7HMzmHZOGPtMzgt8fj2IhkZXq7o3mFWDct0GM7j5ShT3nzFkG8FTalLhPk/htRu3XYojOuWZJoVS0ZGVCkHuP2IJ0EzFEhDaplrXTVTujIEOpdCehlupFcWTCaZ/p8vil646M+JYX9pj6ijihn1XfC/c0w3cO+3neOaBhA+bvISvELI/JOcuUfCHsTv626Fpjw59wv6VcInH $line@Node0" >> /home/"$line"/.ssh/authorized_keys
  
              echo ""  > /home/"$line"/.ssh/id_rsa
              cat > /home/"$line"/.ssh/id_rsa  <<EOF
            -----BEGIN RSA PRIVATE KEY-----
            MIIEogIBAAKCAQEAqSxE3szuoVB4Ii4xzxF1zepJFfbwRamAHOIWujjqFGM7ceup
            gt0m+fdA2b59cG1BWUyRzKkVRuMdPDALDQaQZ7hbBkAoO0cO3KVvLgNtJCaQAFBb
            UyWTvoEruNUBTEi6V9+LU6lwQuxzM5h2Thj7TM4LfH49iIZGV6u6N5hVg3LdBjO4
            +UoU958xZBvBU2pS4T5P4bUbt12KIzrlmSaFUtGRlQpB7j9iCdBMxRIQ2qZa101U
            7oyBDqXQnoZbqRXFkwmmf6fL4peuOjPiWF/aY+oo4oZ9V3wv3NMN3Dvt53jmgYQP
            m7yErxCyPyTnLlHwh7E7+tuhaY8OfcL+lXCJxwIDAQABAoIBADgng6zZJZTSWy4t
            W0c6qnnxfNUXpOXav7XWrmieH8Uos0C7UwcnVZq/of0lKAo7meeEbRkcPv3KwZeK
            8wAd360uGrjWbwROL/a5y0/gv0eyrTYNdmMBJCumQNcXjVi/A2vLvjnFEoiEaDEG
            OK7vx+rUside2BoLSCotzKBLpob8/YyEM0HrKUw/R31SNCKg3IoP49RApXj3ReEk
            8Jry3CzR48vjzu2TnZ74V/V4IN+goehcRTNHX00k6eQj7GhYV5b0TdblAaXZcKjB
            809PQHrUHS5Ub21KB1o4NyQkqIfDJ1R8jdYxrJniFp2fCfW9oj8xv53qmM4B7dXo
            zDWThDECgYEA0GyBNdZNAWbb5e+3hYomyVT0H4N0iyT9MARG3k9SF+TJOtdrWxH9
            2FNXuMiktD/abXp8Coqy0RpsdgyDnpoVr7R0l5jULtTBFx/xrZRNoryHpb/QbuZ7
            fKdWgEDjmFphB1clXUz5Bj0Y+8nIwEns18ULSswGOhzRe9fsM1xiofMCgYEAz8oX
            D27K9EUKhHmewAxUVAQ5RJV2OtXGQKZwlVo3kixRz4eQOrPWpbau6VpboppzCZ4U
            m8JLDYzaEXqeQd5XwebSMfS/+mkxGnRAsvNZIjArIdU8SNZUyUheDuKjPbROyquD
            RHr4Ro15FW3hLog7rHZAL9bmpZeZfoDDNegeGd0CgYBlMTkutWRf2NvM8K0uxdt9
            BqUcI8vSvtu6k2kBCIv4E9lrmymBZuPTQuulSK1G4nWfj8dnqt2Uznp4eizxNShw
            TXIKJGZocl1pZ9YEC6wB5f0KCW4eWgL8i5Zg4KBf2Qmg8buvZ+7EC6f0n4y7Z2j5
            fa602wfu8Qz4TuZcLW+p5wKBgCLCSJdBTlwMTJUajy7LITQovLe3VN7EsfRQo1ao
            j9E47rqLj9nyCX8RDzNj9R4/Pe0m74Wau9lZbYUtANo96mo6RYEr0w19mUQ2nDgT
            Mx7f9ecj94CrseU14N4WlX4V8nQ+uqey9mM++TlXdyrEiU7xPQ2DonOi539c5MrY
            uGhVAoGADSYen+9Z2Diwp7NLh3RwJgyccodyQiK5zmZ9U1nKgt5Gush5mnVCzS6i
            CMBg/5oEw/+R5t0OoVOsI2/S2GA3RNhzGfzLOtiv3pzah9Ma9J6yeH4PwBB8SICl
            jlHv/FXpW8SqRseSJlDCQoXTamhROXK8sKQ18Eb6gvKMRMDYERM=
            -----END RSA PRIVATE KEY-----
            EOF
              
              echo ""  > /home/"$line"/.ssh/config
              cat > /home/"$line"/.ssh/config <<EOF
            Host `echo $MASTER_IP  | sed 's/.[0-9][0-9]*$//g'`.* 0.0.0.0 host-*
               StrictHostKeyChecking no
               UserKnownHostsFile=/dev/null
            EOF

              chown -R "$line":"$line" /home/"$line"/.ssh
              chmod 600 /home/"$line"/.ssh/*
              
              echo "export JAVA_HOME=/usr/lib/jvm/jre" >> /home/"$line"/.bashrc
              echo "export HADOOP_HOME=/usr/lib/${HADOOP_LATEST}" >> /home/"$line"/.bashrc
              echo "export HADOOP_INSTALL=\$HADOOP_HOME" >> /home/"$line"/.bashrc
              echo "export HADOOP_MAPRED_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
              echo "export HADOOP_COMMON_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
              echo "export HADOOP_HDFS_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
              echo "export HADOOP_YARN_HOME=\$HADOOP_HOME" >> /home/"$line"/.bashrc
              echo "export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_HOME/lib/native" >> /home/"$line"/.bashrc
              echo "export PATH=\$PATH:\$HADOOP_HOME/sbin:\$HADOOP_HOME/bin" >> /home/"$line"/.bashrc
              echo "export JAVA_LIBRARY_PATH=\$HADOOP_HOME/lib/native:\$JAVA_LIBRARY_PATH" >> /home/"$line"/.bashrc

              done

            #install iptables
            yum install -y iptables-services
            systemctl enable iptables
            
            # Flush all current rules from iptables#
            iptables -F
            iptables -F -t nat
            iptables --delete-chain
            
            # Set access for localhost
            iptables -A INPUT -i lo -j ACCEPT
            iptables -A OUTPUT -o lo -j ACCEPT
            
            # Subnet open access
            iptables -A INPUT -s $cidr -j ACCEPT
            iptables -A OUTPUT -s $cidr -j ACCEPT

            # web traffic
            iptables -A OUTPUT -p tcp -m tcp --dport 80 -j ACCEPT
            iptables -A OUTPUT -p tcp -m tcp --dport 443 -j ACCEPT

            # DNS
            iptables -A OUTPUT -p udp --dport 53 -m state --state NEW,ESTABLISHED -j ACCEPT
            iptables -A OUTPUT -p tcp --dport 53 -m state --state NEW,ESTABLISHED -j ACCEPT
            
            #drop icmp
            iptables -A INPUT -p icmp --icmp-type any -j DROP
            
            # Accept packets belonging to established and related connections
            iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
            
            # Allow ssh
            iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
            
            # Lock everything down
            iptables -P INPUT DROP
            iptables -P FORWARD DROP
            iptables -P OUTPUT ACCEPT

            #setup hadoop
            yum install -y vim tzdata-java java-1.8.0-openjdk

            tar -zxvf hadoop-*.*.*.tar.gz

            install -Dv /dev/null $HADOOP_CONF_DIR/logs/hadoop_boot.log
            
            HADOOP_LOG='$HADOOP_CONF_DIR/logs/hadoop_boot.log'

            wget -O /etc/yum.repos.d/bigtop.repo www.apache.org/dist/bigtop/bigtop-1.3.0/repos/centos7/bigtop.repo
            yum clean all
            yum install -y bigtop-utils

            ################################################
            #install krb client+server
            yum install -y krb5-workstation krb5-server

            #Set permissions
            chown -R root:hadoop /usr/lib/hadoop /tmp /var
            find /usr/lib/$HADOOP_LATEST -not -path "$HADOOP_CONF_DIR/conf/*" -exec chown root:hadoop {} \;
            chmod -R 770 /usr/lib/$HADOOP_LATEST /usr/lib/hadoop /var 
            chown root:root /var/empty/sshd
            chmod 700 /var/empty/sshd 

            gpasswd -M hadoop,hdfs,yarn,mapred,cc,ccadmin hadoop

            HADOOP_CONF_DIR=/usr/lib/${HADOOP_LATEST}/etc/hadoop
            CORE_SITE_FILE=${HADOOP_CONF_DIR}/core-site.xml
            HDFS_SITE_FILE=${HADOOP_CONF_DIR}/hdfs-site.xml
            MAPRED_SITE_FILE=${HADOOP_CONF_DIR}/mapred-site.xml
            YARN_SITE_FILE=${HADOOP_CONF_DIR}/yarn-site.xml
            WORKERS_FILE=${HADOOP_CONF_DIR}/workers
            POLICY_FILE=${HADOOP_CONF_DIR}/hadoop-policy.xml
       
            ###############################################
            # Fill hadoop conf files     

            echo ""  > $CORE_SITE_FILE
            cat > $CORE_SITE_FILE   <<EOF
            <?xml version="1.0" encoding="UTF-8"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
            <property>
              <name>fs.default.name</name>
                <value>hdfs://$FQDN:9000</value>
            </property>
            <property>
              <name>hadoop.security.authentication</name>
              <value>kerberos</value>
            </property>
            <property>
              <name>hadoop.security.authorization</name>
              <value>true</value>
            </property>
            <property>
              <name>hadoop.security.auth_to_local</name>
              <value>DEFAULT</value>
            </property>
            </configuration>
            EOF
            
            echo ""  > $HDFS_SITE_FILE
            cat > $HDFS_SITE_FILE   <<EOF
            <?xml version="1.0" encoding="UTF-8"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
            <property>
             <name>dfs.replication</name>
             <value>2</value>
            </property>
            <property>
              <name>dfs.name.dir</name>
                <value>file:///usr/lib/hadoop/hadoopdata/hdfs/namenode</value>
            </property>
            <property>
              <name>dfs.data.dir</name>
                <value>file:///usr/lib/hadoop/hadoopdata/hdfs/datanode</value>
            </property>
            <property>
              <name>dfs.block.access.token.enable</name>
              <value>true</value>
            </property>
            
            <!-- NameNode security config -->
            <property>
              <name>dfs.namenode.keytab.file</name>
              <value>$HADOOP_CONF_DIR/conf/hdfs.keytab</value> 
            </property>
            <property>
              <name>dfs.namenode.kerberos.principal</name>
              <value>hdfs/_HOST@HDPCLUSTER.LOCAL</value>
            </property>
            <property>
              <name>dfs.namenode.kerberos.internal.spnego.principal</name>
              <value>HTTP/_HOST@HDPCLUSTER.LOCAL</value>
            </property>
            
            <!-- Secondary NameNode security config -->
            <property>
              <name>dfs.secondary.namenode.keytab.file</name>
              <value>$HADOOP_CONF_DIR/conf/hdfs.keytab</value>
            </property>
            <property>
              <name>dfs.secondary.namenode.kerberos.principal</name>
              <value>hdfs/_HOST@HDPCLUSTER.LOCAL</value>
            </property>
            <property>
              <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
              <value>HTTP/_HOST@HDPCLUSTER.LOCAL</value>
            </property>
            
            <!-- DataNode security config -->
            <property>
              <name>dfs.datanode.data.dir.perm</name>
              <value>700</value> 
            </property>
            <property>
              <name>dfs.datanode.address</name>
              <value>0.0.0.0:1004</value>
            </property>
            <property>
              <name>dfs.datanode.http.address</name>
              <value>0.0.0.0:1006</value>
            </property>
            <property>
              <name>dfs.datanode.keytab.file</name>
              <value>$HADOOP_CONF_DIR/conf/hdfs.keytab</value>
            </property>
            <property>
              <name>dfs.datanode.kerberos.principal</name>
              <value>hdfs/_HOST@HDPCLUSTER.LOCAL</value>
            </property>
            
            <!-- Web Authentication config -->
            <property>
              <name>dfs.web.authentication.kerberos.principal</name>
              <value>HTTP/_HOST@HDPCLUSTER.LOCAL</value>
             </property>
            <property>
              <name>dfs.http.policy</name>
              <value>HTTPS_ONLY</value>
            </property>
            </configuration>
            EOF
            
            echo ""  > $MAPRED_SITE_FILE
            cat > $MAPRED_SITE_FILE   <<EOF
            <configuration>
              <property>
              <name>mapreduce.framework.name</name>
               <value>yarn</value>
             </property>
             <property>
               <name>yarn.app.mapreduce.am.env</name>
                 <value>HADOOP_MAPRED_HOME=/usr/lib/$HADOOP_LATEST</value>
             </property>
             <property>
               <name>mapreduce.map.env</name>
                 <value>HADOOP_MAPRED_HOME=/usr/lib/$HADOOP_LATEST</value>
             </property>
             <property>
               <name>mapreduce.reduce.env</name>
                 <value>HADOOP_MAPRED_HOME=/usr/lib/$HADOOP_LATEST</value>
            </property>
            <!-- MapReduce Job History Server security configs -->
            <property>
              <name>mapreduce.jobhistory.address</name>
              <value>$FQDN:10020</value> 

            </property>
            <property>
              <name>mapreduce.jobhistory.keytab</name>
              <value>$HADOOP_CONF_DIR/conf/mapred.keytab</value>
            <!-- path to the MAPRED keytab for the Job History Server -->
            </property>
            
            <property>
              <name>mapreduce.jobhistory.principal</name>
            
              <value>mapred/_HOST@HDPCLUSTER.LOCAL</value>
            </property>
            
            <!-- To enable TLS/SSL -->
            
            <property>
              <name>mapreduce.jobhistory.http.policy</name>
              <value>HTTPS_ONLY</value>
            </property>
            </configuration>
            EOF
            
            echo ""  > $YARN_SITE_FILE
            cat > $YARN_SITE_FILE  <<EOF
            <?xml version="1.0"?>
            <configuration>
              <property>
                <name>yarn.resourcemanager.resource-tracker.address</name>
                <value>$FQDN:8031</value>
              </property>
              <property>
                <name>yarn.resourcemanager.address</name>
                <value>$FQDN:8032</value>
              </property>
              <property>
                <name>yarn.resourcemanager.scheduler.address</name>
                <value>$FQDN:8030</value>
              </property>
              <property>
                <name>yarn.resourcemanager.admin.address</name>
                <value>$FQDN:8033</value>
              </property>
              <property>
                <name>yarn.resourcemanager.webapp.address</name>
                <value>$FQDN:8088</value>
              </property>
              <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
              </property>
              <property>
                <name>yarn.nodemanager.vmem-check-enabled</name>
                <value>false</value>
              </property>
              <property>
                <name>yarn.nodemanager.pmem-check-enabled</name>
                <value>true</value>
              </property>
              <property>
                <name>yarn.nodemanager.vmem-pmem-ratio</name>
                <value>2.1</value>
              </property>

              <!-- ResourceManager security configs -->
              <property>
                <name>yarn.resourcemanager.keytab</name>
                <value>$HADOOP_CONF_DIR/conf/yarn.keytab</value>
              </property>
              <property>
                <name>yarn.resourcemanager.principal</name>
                <value>yarn/_HOST@HDPCLUSTER.LOCAL</value>
              </property>
             
              <!-- NodeManager security configs -->
              <property>
                <name>yarn.nodemanager.keytab</name>
                <value>$HADOOP_CONF_DIR/conf/yarn.keytab</value>
              <!-- path to the YARN keytab -->
              </property>
              <property>
                <name>yarn.nodemanager.principal</name>
                <value>yarn/_HOST@HDPCLUSTER.LOCAL</value>
              </property>
             
              <property>
                <name>yarn.nodemanager.container-executor.class</name>
             
                <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
              </property>
             
              <property>
                <name>yarn.nodemanager.linux-container-executor.group</name>
                <value>yarn</value>
              </property>

              <property>
                <name>yarn.nodemanager.local-dirs</name>
                <value>/tmp/hadoop-yarn-yarn</value>
              </property>
             
              <!-- To enable TLS/SSL -->
              <property>
                <name>yarn.http.policy</name>
                <value>HTTPS_ONLY</value>
              </property>
            
            </configuration>
            EOF

            ################################################
            #Configure kdc.conf 
            
            cat > /var/kerberos/krb5kdc/kdc.conf <<EOF
            [kdcdefaults]
             kdc_ports = 88
             kdc_tcp_ports = 88
            
            [realms]
             HDPCLUSTER.LOCAL = {
              master_key_type = aes128-cts
              acl_file = /var/kerberos/krb5kdc/kadm5.acl
              dict_file = /usr/share/dict/words
              admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
              supported_enctypes = aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
             }
            EOF
            
            ################################################
            #Configure krb5.conf
            
            cat > /etc/krb5.conf <<EOF
            # Configuration snippets may be placed in this directory as well
            includedir /etc/krb5.conf.d/
            
            [logging]
             default = FILE:/var/log/krb5libs.log
             kdc = FILE:/var/log/krb5kdc.log
             admin_server = FILE:/var/log/kadmind.log
            
            [libdefaults]
             dns_lookup_realm = false
             ticket_lifetime = 24h
             renew_lifetime = 7d
             forwardable = true
             rdns = false
             pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt
             default_realm = HDPCLUSTER.LOCAL
             default_ccache_name = KEYRING:persistent:%{uid}
            
            [realms]
             HDPCLUSTER.LOCAL = {
              kdc = $FQDN
              admin_server = $FQDN
              default_domain = openstacklocal
             }
            
            [domain_realm]
             .openstacklocal = HDPCLUSTER.LOCAL
             openstacklocal = HDPCLUSTER.LOCAL
            EOF

            ################################################
            #Configure hadoop-hdfs-datanode
            
            echo ""  > /etc/default/hadoop-hdfs-datanode
            cat > /etc/default/hadoop-hdfs-datanode <<EOF
            export HADOOP_SECURE_DN_USER=hdfs
            export HADOOP_SECURE_DN_PID_DIR=/var/lib/hadoop-hdfs
            export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs
            export JSVC_HOME=/usr/lib/bigtop-utils/
            EOF

            cat > $HADOOP_CONF_DIR/container-executor.cfg <<EOF
            yarn.nodemanager.local-dirs=/tmp/hadoop-yarn-yarn
            yarn.nodemanager.linux-container-executor.group=yarn
            yarn.nodemanager.log-dirs=/usr/lib/hadoop-3.2.0/logs, /var/log/
            banned.users=bin

            min.user.id=1000
            EOF
            
            ################################################
            #Update realm name in admin conf
            cat > /var/kerberos/krb5kdc/kadm5.acl <<EOF
            */admin@HDPCLUSTER.LOCAL *
            EOF

            ################################################
            #Configure SSH
            cat > /root/.ssh/config <<EOF
            Host `echo $MASTER_IP  | sed 's/.[0-9][0-9]*$//g'`.* 0.0.0.0 host-*
               StrictHostKeyChecking no
               UserKnownHostsFile=/dev/null
            EOF
            
            echo $MASTER_IP $FQDN master >> /etc/hosts
            NODE_COUNT=`cat /tmp/client_ips.txt | tr -d "][," | wc -w`
            for i in $(seq 1 $NODE_COUNT); do
               IP=`cat /tmp/client_ips.txt | tr -d "\n][," | cut -d " " -f $i`
               NODE=`cat /tmp/client_names.txt | tr -d "][," | cut -d " " -f $i`
               echo $IP host-$IP.openstacklocal >> /etc/hosts
            done
            
            echo > $WORKERS_FILE
            for i in $(seq 1 $NODE_COUNT); do
               #NODE_NUM=$((i-1))
               #NODE_NAME=`cat /tmp/client_names.txt | tr -d "][," | cut -d " " -f $i`
               IP=`cat /tmp/client_ips.txt | tr -d "\n][," | cut -d " " -f $i`
               NODE_NAME=host-$IP.openstacklocal
               scp -B -i /home/hadoop/.ssh/id_rsa $CORE_SITE_FILE $YARN_SITE_FILE /etc/hosts /etc/krb5.conf /etc/default/hadoop-hdfs-datanode hadoop@${NODE_NAME}:.
               ssh -i /home/hadoop/.ssh/id_rsa hadoop@${NODE_NAME} 'cat core-site.xml > /usr/lib/hadoop-3.2.0/etc/hadoop/core-site.xml && cat yarn-site.xml > /usr/lib/hadoop-3.2.0/etc/hadoop/yarn-site.xml && cat hosts > /etc/hosts && cat krb5.conf > /etc/krb5.conf && cat hadoop-hdfs-datanode > /etc/default/hadoop-hdfs-datanode'
               echo ${NODE_NAME} >> $WORKERS_FILE
            done

            ################################################
            #Create database           
            runuser -l hadoop -c 'kdb5_util create -r HDPCLUSTER.LOCAL -P hSDfgrfgdsG4554fdwfr42 -s'
            
            ################################################
            #Create admin
            runuser -l hadoop -c 'kadmin.local addprinc -pw fjf425gFREt3tgf3RTfg hadoop/admin@HDPCLUSTER.LOCAL'

            ################################################
            #Create service principals
            runuser -l hadoop -c 'kadmin.local addprinc -randkey hdfs/'${FQDN}'@HDPCLUSTER.LOCAL'
            runuser -l hadoop -c 'kadmin.local addprinc -randkey yarn/'${FQDN}'@HDPCLUSTER.LOCAL'
            runuser -l hadoop -c 'kadmin.local addprinc -randkey mapred/'${FQDN}'@HDPCLUSTER.LOCAL'
            runuser -l hadoop -c 'kadmin.local addprinc -randkey HTTP/'${FQDN}'@HDPCLUSTER.LOCAL'
       
            ################################################
            #Create keytabs
            runuser -l hadoop -c 'kadmin.local xst -norandkey -k hdfs.keytab hdfs/'$FQDN' HTTP/'$FQDN''
            runuser -l hadoop -c 'kadmin.local xst -norandkey -k mapred.keytab mapred/'$FQDN' HTTP/'$FQDN''
            runuser -l hadoop -c 'kadmin.local xst -norandkey -k yarn.keytab yarn/'$FQDN' HTTP/'$FQDN''

            mkdir -m 770 -p $HADOOP_CONF_DIR/conf
            mv /home/hadoop/*.keytab $HADOOP_CONF_DIR/conf
            chown root:hadoop $HADOOP_CONF_DIR/conf
            chown hdfs:hadoop $HADOOP_CONF_DIR/conf/hdfs.keytab
            chown mapred:hadoop $HADOOP_CONF_DIR/conf/mapred.keytab
            chown yarn:hadoop $HADOOP_CONF_DIR/conf/yarn.keytab
            chmod 400 $HADOOP_CONF_DIR/conf/*.keytab
            mkdir -p /usr/lib/$HADOOP_LATEST/logs
            chmod -R 770 /usr/lib/$HADOOP_LATEST/logs
            chgrp -R hadoop /usr/lib/$HADOOP_LATEST/logs

            ################################################
            #Start kerberos
            systemctl start krb5kdc.service
            systemctl start kadmin.service
            systemctl enable krb5kdc.service
            systemctl enable kadmin.service

            ################################################
            # File transfer to clientsqq:
            NODE_COUNT=`cat /tmp/client_ips.txt | tr -d "][," | wc -w`
            for i in $(seq 1 $NODE_COUNT); do
               IP=`cat /tmp/client_ips.txt | tr -d "\n][," | cut -d " " -f $i`
               NODE_NAME=`cat /tmp/client_names.txt | tr -d "][," | cut -d " " -f $i`
               WORKER_FQDN=host-$IP.openstacklocal
               #WORKER_FQDN=$NODE_NAME
               runuser -l hadoop -c 'kadmin.local addprinc -randkey hdfs/'${WORKER_FQDN}'@HDPCLUSTER.LOCAL'
               runuser -l hadoop -c 'kadmin.local addprinc -randkey yarn/'${WORKER_FQDN}'@HDPCLUSTER.LOCAL'
               runuser -l hadoop -c 'kadmin.local addprinc -randkey mapred/'${WORKER_FQDN}'@HDPCLUSTER.LOCAL'
               runuser -l hadoop -c 'kadmin.local addprinc -randkey HTTP/'${WORKER_FQDN}'@HDPCLUSTER.LOCAL'
               runuser -l hadoop -c 'kadmin.local xst -norandkey -k hdfs.keytab hdfs/'$WORKER_FQDN' HTTP/'$WORKER_FQDN''
               runuser -l hadoop -c 'kadmin.local xst -norandkey -k mapred.keytab mapred/'$WORKER_FQDN' HTTP/'$WORKER_FQDN''
               runuser -l hadoop -c 'kadmin.local xst -norandkey -k yarn.keytab yarn/'$WORKER_FQDN' HTTP/'$WORKER_FQDN''
               mkdir -m 770 -p /tmp/"$WORKER_FQDN"
               mv /home/hadoop/*.keytab /tmp/"$WORKER_FQDN"
               
               scp -B -i /home/hadoop/.ssh/id_rsa /tmp/"$WORKER_FQDN"/* hadoop@${WORKER_FQDN}:/tmp
               ssh -i /home/hdfs/.ssh/id_rsa hadoop@${WORKER_FQDN} "chmod 777 /tmp/*.keytab"
               ssh -i /home/hdfs/.ssh/id_rsa hdfs@${WORKER_FQDN} "cp /tmp/hdfs.keytab '$HADOOP_CONF_DIR'/conf/hdfs.keytab && chmod 400 '$HADOOP_CONF_DIR'/conf/hdfs.keytab && kinit -k -t $HADOOP_CONF_DIR/conf/hdfs.keytab hdfs/'$WORKER_FQDN'@HDPCLUSTER.LOCAL"
               ssh -i /home/yarn/.ssh/id_rsa yarn@${WORKER_FQDN} "cp /tmp/yarn.keytab '$HADOOP_CONF_DIR'/conf/yarn.keytab && chmod 400 '$HADOOP_CONF_DIR'/conf/yarn.keytab && kinit -k -t $HADOOP_CONF_DIR/conf/yarn.keytab yarn/'$WORKER_FQDN'@HDPCLUSTER.LOCAL"
               ssh -i /home/mapred/.ssh/id_rsa mapred@${WORKER_FQDN} "cp /tmp/mapred.keytab '$HADOOP_CONF_DIR'/conf/mapred.keytab && chmod 400 '$HADOOP_CONF_DIR'/conf/mapred.keytab && kinit -k -t $HADOOP_CONF_DIR/conf/mapred.keytab mapred/'$WORKER_FQDN'@HDPCLUSTER.LOCAL"
               echo ${WORKER_FQDN} >> $WORKERS_FILE
            done
                        

            cd $CWD

            #Start hadoop services (format first to prevent issues)
            runuser -l hdfs -c 'hdfs namenode -format'
            #rm -rf /tmp/*
            runuser -l hdfs -c 'start-dfs.sh'
            runuser -l hdfs -c "kinit -k -t $HADOOP_CONF_DIR/conf/hdfs.keytab hdfs/'$FQDN'@HDPCLUSTER.LOCAL"
            runuser -l yarn -c "kinit -k -t $HADOOP_CONF_DIR/conf/yarn.keytab yarn/'$FQDN'@HDPCLUSTER.LOCAL"
            runuser -l mapred -c "kinit -k -t $HADOOP_CONF_DIR/conf/mapred.keytab mapred/'$FQDN'@HDPCLUSTER.LOCAL"
            runuser -l hdfs -c 'hadoop fs -mkdir -p /tmp/hadoop-yarn/staging/history/'
            runuser -l hdfs -c 'hadoop fs -chown -R hdfs:hadoop /'
            runuser -l hdfs -c 'hadoop fs -chmod 1777 /tmp'
            runuser -l hdfs -c 'hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging/history/'
            runuser -l yarn -c 'start-yarn.sh'
            runuser -l mapred -c 'mapred --daemon start historyserver'
                        
          params:
            $worker_ips: { get_attr: [hadoop_workers, first_address] }
            $worker_names: { get_attr: [hadoop_workers, name] }
            $cidr: { get_param: network_cidr }
        
  hadoop_master_ip_association:
    type: OS::Nova::FloatingIPAssociation
    properties:
      floating_ip: { get_resource: hadoop_master_floating_ip }
      server_id: { get_resource: hadoop_master }


# The parameters section gathers configuration from the user.
parameters:
  hadoop_worker_count:
    type: number
    description: Number of NFS client instances
    default: 2
    constraints:
      - range: { min: 1 }
        description: There must be at least one client.
  key_name:
    type: string
    description: Name of a KeyPair to enable SSH access to the instance
    constraints:
    - custom_constraint: nova.keypair
  reservation_id:
    type: string
    description: ID of the Blazar reservation to use for launching instances.
    constraints:
    - custom_constraint: blazar.reservation
  network_name:
    type: string
    description: Name of the network to use.
  network_cidr:
    type: string
    description: Cidr of the network
    default: 192.168.100.0/24

outputs:
  hadoop_master_ip:
    description: Public IP address of the Hadoop master
    value: { get_attr: [hadoop_master_floating_ip, ip] }
  hadoop_worker_ips:
    description: Private IP addresses of the Hadoop workers
    value: { get_attr: [hadoop_workers, first_address] }

